{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<a href=\"http://www.inokufu.com\"><img src = \"http://www.inokufu.com/wp-content/uploads/elementor/thumbs/logo_inokufu_vector_full-black-om2hmu9ob1jytetxemkj1ij8g7tt3hzrtssivh2fl2.png\" width = 400> </a>\n",
    "\n",
    "\n",
    "<h1 align=center><font size = 5>Exploratory Data Analysis : Titre</font></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Introduction\n",
    "\n",
    "In this notebook, we conduct an Exploratory Data Analysis (EDA). The idea is to create different Word2Vec models. The idea is to create as much models as we want, in order to see which variables are the most interesting for the models, in order to try them on our classification problem. \n",
    "\n",
    "Our EDA approach follows the **Data Science Methodology CRISP-DM**. For more info about this approach, check this [Wikipedia page](https://en.wikipedia.org/wiki/Cross-industry_standard_process_for_data_mining)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Table of Contents\n",
    "\n",
    "<div class=\"alert alert-block alert-info\" style=\"margin-top: 20px\">\n",
    "\n",
    "<font size = 3>\n",
    "\n",
    "1. <a href=\"#item1\">Data Import</a>\n",
    "\n",
    "2. <a href=\"#item2\">Creation of models</a>\n",
    "\n",
    "3. <a href=\"#item3\">Next Steps</a>    \n",
    "\n",
    "</font>\n",
    "</div>\n",
    "<a id='the_destination'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Import <a id='item1'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "np.set_printoptions(threshold=10000,suppress=True) \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.colors as colors\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import matplotlib.image as img\n",
    "from matplotlib import rcParams\n",
    "\n",
    "import json\n",
    "import unicodedata\n",
    "\n",
    "import seaborn as sns\n",
    "from cycler import cycler\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import spacy\n",
    "import nltk\n",
    "import os\n",
    "import string\n",
    "import numpy as np\n",
    "import copy\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import re\n",
    "import math\n",
    "\n",
    "import fr_core_news_sm\n",
    "from spacy_langdetect import LanguageDetector\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from collections import Counter\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, recall_score, precision_score, roc_auc_score\n",
    "\n",
    "import gensim\n",
    "import time \n",
    "\n",
    "import multiprocess\n",
    "import multiprocessing\n",
    "\n",
    "print('Libraries imported.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Import <a id='item1'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Openning all the treated data files\n",
    "\n",
    "#path = './data/20200330_Processed_Data/'\n",
    "path = './data/20200408_Processed_Data/'\n",
    "\n",
    "# file_name_list = ['20200327_Processed_Udemy_Descriptions_Tokens', \n",
    "#                   '20200327_Processed_Descriptions_Tokens', \n",
    "#                   '20200327_Processed_Objectives_Tokens']\n",
    "file_name_list = ['20200408_Processed_Udemy_Descriptions_Tokens', \n",
    "                  '20200408_Processed_Descriptions_Tokens', \n",
    "                  '20200408_Processed_Objectives_Tokens',\n",
    "                  \n",
    "                  '20200410_Processed_Descriptions_Tokens_NewLO',\n",
    "                  '20200410_Processed_Objectives_Tokens_NewLO',\n",
    "                 ]\n",
    "\n",
    "file_extension = '.csv'\n",
    "\n",
    "frames = []\n",
    "\n",
    "# For each file in the list\n",
    "for file_name in file_name_list:\n",
    "    # Reading the file as a CSV file\n",
    "    df = pd.read_csv( path + file_name + file_extension, sep=\";\")\n",
    "    \n",
    "    # Keeping only the second columns (first column of each file contains indexes and the second one contains data)\n",
    "    df['all'] = df[df.columns[1]]\n",
    "    \n",
    "    # Add it to the frames list\n",
    "    frames.append(df)\n",
    "\n",
    "# Concat all the df from the frames list\n",
    "df_all = pd.concat(frames)\n",
    "\n",
    "# Keeping only the column that contains all the processed data\n",
    "final_corpus = df_all['all']\n",
    "\n",
    "# Reseting indexes\n",
    "final_corpus = final_corpus.reset_index(drop=True)\n",
    "\n",
    "# Applying the tokenization\n",
    "final_corpus = final_corpus.apply(lambda line: gensim.utils.simple_preprocess(line))\n",
    "\n",
    "final_corpus.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_corpus.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Creation of models <a id='item2'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cores = multiprocessing.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.Word2Vec(final_corpus, size = model_size, window = 5, min_count = 2, \n",
    "                               workers = cores-1, sg = 0, max_final_vocab = 5000, iter=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(model.wv.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "for i in range(25):\n",
    "    model.train(final_corpus, total_examples=len(final_corpus), epochs=5)\n",
    "    print('Train #', i , end=\"\\r\")\n",
    "print(\"Elapsed time (seconds) to train Word2Vec: \", time.time() - t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.save(\"./models/20200410_UdemyDesc_Desc_Obj_100_5000_Train25_Iter10_lemma_newLO.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We saved 5 models : \n",
    "- 20200330_UdemyDesc_Desc_Obj_100_5000_Train25_Iter10\n",
    "    - ModelSize = 100\n",
    "    - VocabSize = 5k\n",
    "    - Train = 25\n",
    "    - Iter = 10\n",
    "    - Trained on data transformed using a STEMMER\n",
    "- 20200330_UdemyDesc_Desc_Obj_300_5000_Train25_Iter10\n",
    "    - ModelSize = 300\n",
    "    - VocabSize = 5k\n",
    "    - Train = 25\n",
    "    - Iter = 10\n",
    "    - Trained on data transformed using a STEMMER\n",
    "- 20200330_UdemyDesc_Desc_Obj_100_10000_Train25_Iter10\n",
    "    - ModelSize = 100\n",
    "    - VocabSize = 10k\n",
    "    - Train = 25\n",
    "    - Iter = 10\n",
    "    - Trained on data transformed using a STEMMER\n",
    "- 20200330_UdemyDesc_Desc_Obj_300_10000_Train25_Iter10\n",
    "    - ModelSize = 300\n",
    "    - VocabSize = 10k\n",
    "    - Train = 25\n",
    "    - Iter = 10\n",
    "    - Trained on data transformed using a STEMMER\n",
    "- 20200408_UdemyDesc_Desc_Obj_100_5000_Train25_Iter10_lemma\n",
    "    - ModelSize = 100\n",
    "    - VocabSize = 5k\n",
    "    - Train = 25\n",
    "    - Iter = 10\n",
    "    - Trained on data transformed using a LEMMATIZER\n",
    "- 20200408_UdemyDesc_Desc_Obj_300_5000_Train25_Iter10_lemma\n",
    "    - ModelSize = 300\n",
    "    - VocabSize = 5k\n",
    "    - Train = 25\n",
    "    - Iter = 10\n",
    "    - Trained on data transformed using a LEMMATIZER\n",
    "- 20200410_UdemyDesc_Desc_Obj_300_5000_Train25_Iter10_lemma_newLO\n",
    "    - ModelSize = 300\n",
    "    - VocabSize = 5k\n",
    "    - Train = 25\n",
    "    - Iter = 10\n",
    "    - Trained on data transformed using a LEMMATIZER and with the new LO's\n",
    "- 20200410_UdemyDesc_Desc_Obj_300_10000_Train25_Iter10_lemma_newLO\n",
    "    - ModelSize = 300\n",
    "    - VocabSize = 10k\n",
    "    - Train = 25\n",
    "    - Iter = 10\n",
    "    - Trained on data transformed using a LEMMATIZER and with the new LO's\n",
    "- 20200410_UdemyDesc_Desc_Obj_300_5000_Train25_Iter10_lemma_newLO\n",
    "    - ModelSize = 100\n",
    "    - VocabSize = 5k\n",
    "    - Train = 25\n",
    "    - Iter = 10\n",
    "    - Trained on data transformed using a LEMMATIZER and with the new LO's"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Next Steps <a id='item3'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NB : This notebook is not finished : we want trop try other variables changes, and we also wanted to add the titles to the corpus. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we created, trained, and saved 4 differents models. \n",
    "We tried to change 2 variables : \n",
    "- Model Size\n",
    "- Vocab Size \n",
    "The next step is to test these models in order to see which one is the most accuracate and precise for our classification problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<hr>\n",
    "\n",
    "Author [Guillaume Lefebvre](https://www.linkedin.com/in/guillaume-lefebvre-22117610b/) - For more information, contact us at contact@inokufu.com - Copyright &copy; 2020 [Inokufu](http://www.inokufu.com)\n",
    "\n",
    "<a href=\"http://www.inokufu.com\"><img src = \"http://www.inokufu.com/wp-content/uploads/elementor/thumbs/logo_inokufu_vector_full-black-om2hmu9ob1jytetxemkj1ij8g7tt3hzrtssivh2fl2.png\" width = 400> </a>\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
