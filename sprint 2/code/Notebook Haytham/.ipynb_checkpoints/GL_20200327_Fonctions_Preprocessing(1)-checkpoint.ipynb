{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<a href=\"http://www.inokufu.com\"><img src = \"http://www.inokufu.com/wp-content/uploads/elementor/thumbs/logo_inokufu_vector_full-black-om2hmu9ob1jytetxemkj1ij8g7tt3hzrtssivh2fl2.png\" width = 400> </a>\n",
    "\n",
    "\n",
    "<h1 align=center><font size = 5>Exploratory Data Analysis : Data Processing</font></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this notebook, we keep the different functions that we will have to use for data processing.\n",
    "\n",
    "Our EDA approach follows the **Data Science Methodology CRISP-DM**. For more info about this approach, check this [Wikipedia page](https://en.wikipedia.org/wiki/Cross-industry_standard_process_for_data_mining)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "np.set_printoptions(threshold=10000,suppress=True) \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.colors as colors\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import matplotlib.image as img\n",
    "from matplotlib import rcParams\n",
    "\n",
    "import json\n",
    "import unicodedata\n",
    "\n",
    "import seaborn as sns\n",
    "from cycler import cycler\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import spacy\n",
    "import nltk\n",
    "import os\n",
    "import string\n",
    "import numpy as np\n",
    "import copy\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import re\n",
    "import math\n",
    "\n",
    "import fr_core_news_sm\n",
    "from spacy_langdetect import LanguageDetector\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from collections import Counter\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, recall_score, precision_score, roc_auc_score\n",
    "\n",
    "import gensim\n",
    "import time \n",
    "\n",
    "import multiprocess\n",
    "import multiprocessing\n",
    "\n",
    "import import_ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = fr_core_news_sm.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_urls (data):\n",
    "    data = re.sub(r'(https|http)?:\\/\\/(\\w|\\.|\\/|\\?|\\=|\\&|\\%)*\\b', ' ', data, flags=re.MULTILINE)\n",
    "    return data\n",
    "\n",
    "def remove_html(data):\n",
    "    return BeautifulSoup(data).get_text(separator=\" \").strip()\n",
    "\n",
    "def remove_quote(data):\n",
    "    return data.replace(\"'\",\" \")\n",
    "\n",
    "def remove_special_quote(data):\n",
    "    return data.replace(\"’\",\" \")\n",
    "\n",
    "def remove_back_quote(data):\n",
    "    return data.replace(\"`\",\" \")\n",
    "\n",
    "def remove_multiple_space(data):\n",
    "    return ' '.join(data.split())\n",
    "\n",
    "def remove_interrogation_reverse(data):\n",
    "    return data.replace(\"¿\",\" \")\n",
    "\n",
    "def convert_lower_case(data):\n",
    "    return np.char.lower(data)\n",
    "\n",
    "def remove_antislash(data):\n",
    "    symbols = [\"\\n\", \"\\t\", \"\\r\"]\n",
    "    for i in range(len(symbols)):\n",
    "        data = data.replace(symbols[i],\" \")\n",
    "    return data\n",
    "\n",
    "def remove_accents(data):\n",
    "    data = ''.join((c for c in unicodedata.normalize('NFD', data) if unicodedata.category(c) != 'Mn'))\n",
    "    return data\n",
    "\n",
    "def remove_punctuation(data):\n",
    "    tokens = word_tokenize(str(data))\n",
    "    new_text = \"\"\n",
    "    for w in tokens:\n",
    "        new_string = re.sub('[^a-zA-Z]+', '', w)\n",
    "        if len(new_string) > 0:\n",
    "            new_text = new_text + \" \" + new_string\n",
    "    return new_text\n",
    "\n",
    "def stemming(data):\n",
    "    stemmer = SnowballStemmer('french')\n",
    "    tokens = word_tokenize(str(data))\n",
    "    new_text = \"\"\n",
    "    for w in tokens:\n",
    "        new_text = new_text + \" \" + stemmer.stem(w)\n",
    "    return new_text\n",
    "\n",
    "def remove_stop_words(data):\n",
    "    stop_words = stopwords.words('french')\n",
    "    words = word_tokenize(str(data))\n",
    "    new_text = \"\"\n",
    "    for w in words:\n",
    "        if w not in stop_words:\n",
    "            if len(w.strip()) > 2:\n",
    "                new_text = new_text + \" \" + w\n",
    "    return new_text\n",
    "\n",
    "def remove_small_words(data):\n",
    "    words = word_tokenize(str(data))\n",
    "    new_text = \"\"\n",
    "    for w in words:\n",
    "        if len(w.strip()) > 2:\n",
    "            new_text = new_text + \" \" + w\n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(string_to_test):\n",
    "    string_to_test = str(remove_urls(string_to_test))\n",
    "    string_to_test = str(remove_html(string_to_test))\n",
    "    string_to_test = str(remove_antislash(string_to_test))\n",
    "    \n",
    "    string_to_test = str(convert_lower_case(string_to_test))\n",
    "    \n",
    "    string_to_test = str(remove_quote(string_to_test))\n",
    "    string_to_test = str(remove_special_quote(string_to_test))\n",
    "    string_to_test = str(remove_back_quote(string_to_test))\n",
    "    string_to_test = str(remove_interrogation_reverse(string_to_test))\n",
    "    string_to_test = str(remove_multiple_space(string_to_test))\n",
    "    \n",
    "    string_to_test = str(remove_accents(string_to_test))\n",
    "    string_to_test = str(remove_punctuation(string_to_test))\n",
    "    \n",
    "    string_to_test = str(remove_stop_words(string_to_test))\n",
    "    \n",
    "    string_to_test = str(string_to_test.strip())\n",
    "    \n",
    "    string_to_test = str(remove_small_words(string_to_test))\n",
    "    string_to_test = str(stemming(string_to_test))\n",
    "    \n",
    "    string_to_test = str(string_to_test.strip())\n",
    "    \n",
    "    return string_to_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_lemma(string_to_test):\n",
    "    string_to_test = str(remove_urls(string_to_test))\n",
    "    string_to_test = str(remove_html(string_to_test))\n",
    "    string_to_test = str(remove_antislash(string_to_test))\n",
    "    \n",
    "    string_to_test = str(convert_lower_case(string_to_test))\n",
    "    \n",
    "    string_to_test = str(remove_quote(string_to_test))\n",
    "    string_to_test = str(remove_special_quote(string_to_test))\n",
    "    string_to_test = str(remove_back_quote(string_to_test))\n",
    "    string_to_test = str(remove_interrogation_reverse(string_to_test))\n",
    "    string_to_test = str(remove_multiple_space(string_to_test))\n",
    "    \n",
    "    doc = nlp(string_to_test)\n",
    "    new_string = \"\"\n",
    "    for token in doc:\n",
    "        new_string += token.lemma_ + \" \"\n",
    "    string_to_test = new_string\n",
    "    \n",
    "    string_to_test = str(remove_accents(string_to_test))\n",
    "    string_to_test = str(remove_punctuation(string_to_test))\n",
    "    \n",
    "    string_to_test = str(remove_stop_words(string_to_test))\n",
    "    \n",
    "    string_to_test = str(string_to_test.strip())\n",
    "    \n",
    "    string_to_test = str(remove_small_words(string_to_test))\n",
    "    string_to_test = str(stemming(string_to_test))\n",
    "    \n",
    "    string_to_test = str(string_to_test.strip())\n",
    "    \n",
    "    return string_to_test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<hr>\n",
    "\n",
    "Author [Guillaume Lefebvre](https://www.linkedin.com/in/guillaume-lefebvre-22117610b/) - For more information, contact us at contact@inokufu.com - Copyright &copy; 2020 [Inokufu](http://www.inokufu.com)\n",
    "\n",
    "<a href=\"http://www.inokufu.com\"><img src = \"http://www.inokufu.com/wp-content/uploads/elementor/thumbs/logo_inokufu_vector_full-black-om2hmu9ob1jytetxemkj1ij8g7tt3hzrtssivh2fl2.png\" width = 400> </a>\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
