{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<a href=\"http://www.inokufu.com\"><img src = \"http://www.inokufu.com/wp-content/uploads/elementor/thumbs/logo_inokufu_vector_full-black-om2hmu9ob1jytetxemkj1ij8g7tt3hzrtssivh2fl2.png\" width = 400> </a>\n",
    "\n",
    "\n",
    "<h1 align=center><font size = 5>Exploratory Data Analysis : Titre</font></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this notebook, we conduct an Exploratory Data Analysis (EDA) of data about data preprocessing collected from [udemy](https://udemy.com)'s API and the LO files. The idea is to better understand how to treat data correctly in order to give tokenized and cleaned data to Word2Vec models. \n",
    "\n",
    "Our EDA approach follows the **Data Science Methodology CRISP-DM**. For more info about this approach, check this [Wikipedia page](https://en.wikipedia.org/wiki/Cross-industry_standard_process_for_data_mining)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "<div class=\"alert alert-block alert-info\" style=\"margin-top: 20px\">\n",
    "\n",
    "<font size = 3>\n",
    "\n",
    "1. <a href=\"#item1\">Data Collection</a>\n",
    "\n",
    "2. <a href=\"#item2\">Creation of models</a>\n",
    "\n",
    "3. <a href=\"#item3\">Conclusion</a>    \n",
    "\n",
    "</font>\n",
    "</div>\n",
    "<a id='the_destination'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Collection <a id='item1'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "np.set_printoptions(threshold=10000,suppress=True) \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.colors as colors\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import matplotlib.image as img\n",
    "from matplotlib import rcParams\n",
    "\n",
    "import json\n",
    "import unicodedata\n",
    "\n",
    "import seaborn as sns\n",
    "from cycler import cycler\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import spacy\n",
    "import nltk\n",
    "import os\n",
    "import string\n",
    "import numpy as np\n",
    "import copy\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import re\n",
    "import math\n",
    "\n",
    "import fr_core_news_sm\n",
    "from spacy_langdetect import LanguageDetector\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from collections import Counter\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, recall_score, precision_score, roc_auc_score\n",
    "\n",
    "import gensim\n",
    "import time \n",
    "\n",
    "import multiprocess\n",
    "import multiprocessing\n",
    "\n",
    "import import_ipynb\n",
    "from GL_20200327_Fonctions_Preprocessing import remove_urls, remove_html, remove_antislash, convert_lower_case\n",
    "from GL_20200327_Fonctions_Preprocessing import remove_quote, remove_back_quote, remove_interrogation_reverse\n",
    "from GL_20200327_Fonctions_Preprocessing import remove_accents, remove_punctuation, remove_stop_words \n",
    "from GL_20200327_Fonctions_Preprocessing import remove_small_words, stemming, preprocess, preprocess_lemma \n",
    "\n",
    "print('Libraries imported.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Udemy data from Json files (coming from Udemy's API)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Openning jsons files containing udemy's data\n",
    "\n",
    "udemy_json = []\n",
    "for page in range(1,27):\n",
    "    page = page * 10\n",
    "    \n",
    "    with open('./data/20200317_Udemy_FR/20200317_Udemy_FR_'+str(page)+'.json') as f:\n",
    "        udemy_json = udemy_json + json.load(f)\n",
    "\n",
    "with open('./data/20200317_Udemy_FR/20200317_Udemy_FR_264.json') as f:\n",
    "    udemy_json = udemy_json + json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare empty arrays of every variables we want to keep\n",
    "desc, title, rating_dist, price_detail, avg_rating, num_sub = [],[],[],[],[],[]\n",
    "\n",
    "# for each item of the array of json\n",
    "for result in udemy_json:\n",
    "    \n",
    "    # Choosing only the variables that we need\n",
    "    desc.append(result['description'])\n",
    "    # head.append(result['headline'])\n",
    "    # rating_dist.append(result['rating_distribution'])\n",
    "    avg_rating.append(result['avg_rating'])\n",
    "    title.append(result['title'])\n",
    "    num_sub.append(result['num_subscribers'])\n",
    "\n",
    "    if result['price_detail']:\n",
    "        price_detail.append(result['price_detail']['amount'])\n",
    "    else:\n",
    "        price_detail.append(0.0)\n",
    "    \n",
    "# Creation of a dataframe containing all these variables \n",
    "df_json = pd.DataFrame([desc, price_detail, title, num_sub, avg_rating]).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Further ameliorations : add some other variables \n",
    "df_json.columns = [\"description\", \"price_detail\", \"title\", \"num_sub\", \"avg_rating\"]\n",
    "df_json.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing null average ratings, because it is usually not filled and not a bad mark on purpose\n",
    "df_json = df_json.loc[df_json['avg_rating'] != 0]\n",
    "\n",
    "# Reseting indexes\n",
    "df_json = df_json.reset_index(drop=True)\n",
    "\n",
    "# Creating new column, and filling it by NaN\n",
    "df_json = df_json.assign(counter='Nan')\n",
    "\n",
    "# For each line in the dataframe of the udemy data\n",
    "for index,row in df_json.iterrows():\n",
    "    \n",
    "    # Preprocessing the description --> stemmer\n",
    "    preprocessed_data = preprocess(row['description'])\n",
    "    df_json.at[index,'stem_description'] = str(preprocessed_data)\n",
    "    \n",
    "    # Preprocessing the description --> lemmatizer\n",
    "    preprocessed_data = preprocess_lemma(row['description'])\n",
    "    df_json.at[index,'lemma_description'] = str(preprocessed_data)\n",
    "    \n",
    "    # Preprocessing the title, and replace the classic title by the processed title\n",
    "    preprocessed_title = preprocess(row['title'])\n",
    "    df_json.at[index,'title'] = str(preprocessed_title)\n",
    "    \n",
    "    # Calculating the number of occurances of words in the processed description, and add it to the DF\n",
    "    frequence = Counter(word_tokenize(str(preprocessed_data)))\n",
    "    df_json.at[index,'counter'] = frequence\n",
    "    \n",
    "    # Splitting avg_rating into 2 values to do some classification, and add it to the rating_01\n",
    "    if row['avg_rating'] >= 4.252:\n",
    "        value = int(1)\n",
    "    else:\n",
    "        value = int(0)\n",
    "        \n",
    "    df_json.at[index,'rating_01'] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(df_json['rating_01'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_json.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting data into \n",
    "corpus = df_json['lemma_description']\n",
    "title = df_json['title']\n",
    "Y = df_json['rating_01'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrinting df json treated to a file\n",
    "\n",
    "# Converting ratings from float to int\n",
    "df_json['rating_01'] = df_json['rating_01'].astype(int)\n",
    "\n",
    "begin = time.time()\n",
    "df_json.to_csv('./data/20200408_Processed_Data/20200408_Processed_Udemy_Json.csv',sep=';')\n",
    "end = time.time()\n",
    "\n",
    "print(\"Done in {} seconds\".format(round(end-begin,2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Formations data from LO csv files (coming from extract of DB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Openning all the formations files\n",
    "path = './data/20200308 LO/LO'\n",
    "file_extension = '.csv'\n",
    "\n",
    "frames = []\n",
    "keys = []\n",
    "\n",
    "for i in range(1,15):\n",
    "    df = pd.read_csv(path+str(i)+file_extension)\n",
    "    frames.append(df)\n",
    "\n",
    "df_formation = pd.concat(frames)\n",
    "df_formation.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Processing Descriptions of Formations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keeping only the descriptions, and only unique values\n",
    "\n",
    "desc = df_formation.loc[:,['description']]\n",
    "desc = desc.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing null descriptions, and reseting indexes \n",
    "\n",
    "desc = desc.loc[desc['description'].isnull() == False]\n",
    "desc = desc.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processing all the descriptions \n",
    "count = 0\n",
    "\n",
    "# For each descriptions in the desc DF\n",
    "for index,row in desc.iterrows():\n",
    "    \n",
    "    if count % 50 == 0:\n",
    "        # Calculating the percentage of processed data \n",
    "        calcul_percent = round((count/len(desc))*100,2)\n",
    "        print(\"Loading...\",calcul_percent,\"%\",end=\"\\r\")\n",
    "    count = count + 1\n",
    "    \n",
    "    # Processing descriptions of formations\n",
    "    preprocessed_data = preprocess(row['description'])\n",
    "    preprocessed_lemma_data = preprocess_lemma(row['description'])\n",
    "    desc.at[index,'description_bis'] = str(preprocessed_data)\n",
    "    desc.at[index,'description_ter'] = str(preprocessed_lemma_data)\n",
    "    \n",
    "print(\"Loading... 100.00 %\",end=\"\\r\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Processing Objectives of Formations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keeping only the objectives, and only unique values\n",
    "\n",
    "obj = df_formation.loc[:,['objectifs']]\n",
    "obj = obj.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing null objectives, and reseting indexes \n",
    "\n",
    "obj = obj.loc[obj['objectifs'].isnull() == False]\n",
    "obj = obj.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processing all the objectives \n",
    "count = 0\n",
    "\n",
    "for index,row in obj.iterrows():\n",
    "    \n",
    "    if count % 50 == 0:\n",
    "        # Calculating the percentage of processed data \n",
    "        calcul_percent = round((count/len(desc))*100,2)\n",
    "        print(\"Loading...\",calcul_percent,\"%\",end=\"\\r\")\n",
    "    count = count + 1\n",
    "    \n",
    "    # Processing objectives of formations\n",
    "    preprocessed_data = preprocess(row['objectifs'])\n",
    "    preprocessed_lemma_data = preprocess_lemma(row['objectifs'])\n",
    "    obj.at[index,'objectifs_bis'] = str(preprocessed_data)\n",
    "    obj.at[index,'objectifs_ter'] = str(preprocessed_lemma_data)\n",
    "\n",
    "print(\"Loading... 100.00 %\",end=\"\\r\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Keeping only the processed data and removing duplicated and null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keeping only the processed data\n",
    "#desc_formation = desc['description_bis']\n",
    "#obj_formation = obj['objectifs_bis']\n",
    "\n",
    "desc_formation = desc['description_ter']\n",
    "obj_formation = obj['objectifs_ter']\n",
    "\n",
    "# Removing null values \n",
    "desc_formation = desc_formation.loc[desc_formation.isnull() == False]\n",
    "obj_formation = obj_formation.loc[obj_formation.isnull() == False]\n",
    "\n",
    "# Dropping duplacted values\n",
    "desc_formation = desc_formation.drop_duplicates()\n",
    "obj_formation = obj_formation.drop_duplicates()\n",
    "\n",
    "# Reseting indexes \n",
    "desc_formation = desc_formation.reset_index(drop=True)\n",
    "obj_formation = obj_formation.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Writing processed data to files containing Text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Wrinting processed descriptions (still as a text) to a file\n",
    "\n",
    "begin = time.time()\n",
    "desc_formation.to_csv('./data/20200408_Processed_Data/20200408_Processed_Descriptions_Text.csv',sep=';')\n",
    "end = time.time()\n",
    "\n",
    "print(\"Done in {} seconds\".format(round(end-begin,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrinting processed objectives (still as a text) to a file\n",
    "\n",
    "begin = time.time()\n",
    "obj_formation.to_csv('./data/20200408_Processed_Data/20200408_Processed_Objectives_Text.csv',sep=';')\n",
    "end = time.time()\n",
    "\n",
    "print(\"Done in {} seconds\".format(round(end-begin,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrinting processed udemy's descriptions (still as a text) to a file\n",
    "\n",
    "begin = time.time()\n",
    "corpus.to_csv('./data/20200408_Processed_Data/20200408_Processed_Udemy_Descriptions_Text.csv',sep=';')\n",
    "end = time.time()\n",
    "\n",
    "print(\"Done in {} seconds\".format(round(end-begin,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrinting processed udemy's titles (still as a text) to a file\n",
    "\n",
    "begin = time.time()\n",
    "title.to_csv('./data/20200408_Processed_Data/20200408_Processed_Udemy_Title_Text.csv',sep=';')\n",
    "end = time.time()\n",
    "\n",
    "print(\"Done in {} seconds\".format(round(end-begin,2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenizing data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simply processing data with gensim function \n",
    "# --> removing stop words + tokenization \n",
    "\n",
    "corpus = corpus.apply(lambda line: gensim.utils.simple_preprocess(line))\n",
    "title = title.apply(lambda line: gensim.utils.simple_preprocess(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simply processing data with gensim function \n",
    "# --> removing stop words + tokenization \n",
    "\n",
    "desc_formation = desc_formation.apply(lambda line: gensim.utils.simple_preprocess(line))\n",
    "obj_formation = obj_formation.apply(lambda line: gensim.utils.simple_preprocess(line))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Concatening all treated and tokenized dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate all treated dataframes to a final corpus dataframe, and reseting indexes\n",
    "\n",
    "final_corpus = pd.concat([desc_formation, obj_formation, corpus])\n",
    "final_corpus = final_corpus.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "final_corpus.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Writing processed data to files containing Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrinting processed corpus (as tokens) to a file\n",
    "\n",
    "begin = time.time()\n",
    "corpus.to_csv('./data/20200408_Processed_Data/20200408_Processed_Udemy_Descriptions_Tokens.csv',sep=';')\n",
    "end = time.time()\n",
    "\n",
    "print(\"Done in {} seconds\".format(round(end-begin,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrinting processed title (as tokens) to a file\n",
    "\n",
    "begin = time.time()\n",
    "title.to_csv('./data/20200408_Processed_Data/20200408_Processed_Udemy_Titles_Tokens.csv',sep=';')\n",
    "end = time.time()\n",
    "\n",
    "print(\"Done in {} seconds\".format(round(end-begin,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrinting processed descriptions (as tokens) to a file\n",
    "\n",
    "begin = time.time()\n",
    "desc_formation.to_csv('./data/20200408_Processed_Data/20200408_Processed_Descriptions_Tokens.csv',sep=';')\n",
    "end = time.time()\n",
    "\n",
    "print(\"Done in {} seconds\".format(round(end-begin,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrinting processed objectives (still as a text) to a file\n",
    "\n",
    "begin = time.time()\n",
    "obj_formation.to_csv('./data/20200408_Processed_Data/20200408_Processed_Objectives_Tokens.csv',sep=';')\n",
    "end = time.time()\n",
    "\n",
    "print(\"Done in {} seconds\".format(round(end-begin,2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New LO from CPF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Openning all the formations files\n",
    "path = './data/'\n",
    "file_name = '20200407_LO_MonCompteFormation'\n",
    "file_extension = '.csv'\n",
    "\n",
    "df = pd.read_csv(path+file_name+file_extension)\n",
    "\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keeping only the descriptions, and only unique values\n",
    "\n",
    "desc = df.loc[:,['description']]\n",
    "desc = desc.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing null descriptions, and reseting indexes \n",
    "\n",
    "desc = desc.loc[desc['description'].isnull() == False]\n",
    "desc = desc.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processing all the descriptions \n",
    "count = 0\n",
    "\n",
    "# For each descriptions in the desc DF\n",
    "for index,row in desc.iterrows():\n",
    "    \n",
    "    if count % 50 == 0:\n",
    "        # Calculating the percentage of processed data \n",
    "        calcul_percent = round((count/len(desc))*100,2)\n",
    "        print(\"Loading...\",calcul_percent,\"%\",end=\"\\r\")\n",
    "    count = count + 1\n",
    "    \n",
    "    # Processing descriptions of formations\n",
    "    preprocessed_data = preprocess(row['description'])\n",
    "    preprocessed_lemma_data = preprocess_lemma(row['description'])\n",
    "    desc.at[index,'description_bis'] = str(preprocessed_data)\n",
    "    desc.at[index,'description_ter'] = str(preprocessed_lemma_data)\n",
    "    \n",
    "print(\"Loading... 100.00 %\",end=\"\\r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keeping only the objectives, and only unique values\n",
    "\n",
    "obj = df.loc[:,['objectifs']]\n",
    "obj = obj.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing null objectives, and reseting indexes \n",
    "\n",
    "obj = obj.loc[obj['objectifs'].isnull() == False]\n",
    "obj = obj.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processing all the objectives \n",
    "count = 0\n",
    "\n",
    "for index,row in obj.iterrows():\n",
    "    \n",
    "    if count % 50 == 0:\n",
    "        # Calculating the percentage of processed data \n",
    "        calcul_percent = round((count/len(desc))*100,2)\n",
    "        print(\"Loading...\",calcul_percent,\"%\",end=\"\\r\")\n",
    "    count = count + 1\n",
    "    \n",
    "    # Processing objectives of formations\n",
    "    preprocessed_data = preprocess(row['objectifs'])\n",
    "    preprocessed_lemma_data = preprocess_lemma(row['objectifs'])\n",
    "    obj.at[index,'objectifs_bis'] = str(preprocessed_data)\n",
    "    obj.at[index,'objectifs_ter'] = str(preprocessed_lemma_data)\n",
    "\n",
    "print(\"Loading... 100.00 %\",end=\"\\r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keeping only the processed data\n",
    "#desc_formation = desc['description_bis']\n",
    "#obj_formation = obj['objectifs_bis']\n",
    "\n",
    "desc_formation = desc['description_ter']\n",
    "obj_formation = obj['objectifs_ter']\n",
    "\n",
    "# Removing null values \n",
    "desc_formation = desc_formation.loc[desc_formation.isnull() == False]\n",
    "obj_formation = obj_formation.loc[obj_formation.isnull() == False]\n",
    "\n",
    "# Dropping duplacted values\n",
    "desc_formation = desc_formation.drop_duplicates()\n",
    "obj_formation = obj_formation.drop_duplicates()\n",
    "\n",
    "# Reseting indexes \n",
    "desc_formation = desc_formation.reset_index(drop=True)\n",
    "obj_formation = obj_formation.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrinting processed descriptions (still as a text) to a file\n",
    "\n",
    "begin = time.time()\n",
    "desc_formation.to_csv('./data/20200408_Processed_Data/20200410_Processed_Descriptions_Text_NewLO.csv',sep=';')\n",
    "end = time.time()\n",
    "\n",
    "print(\"Done in {} seconds\".format(round(end-begin,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrinting processed objectives (still as a text) to a file\n",
    "\n",
    "begin = time.time()\n",
    "obj_formation.to_csv('./data/20200408_Processed_Data/20200410_Processed_Objectives_Text_NewLO.csv',sep=';')\n",
    "end = time.time()\n",
    "\n",
    "print(\"Done in {} seconds\".format(round(end-begin,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simply processing data with gensim function \n",
    "# --> removing stop words + tokenization \n",
    "\n",
    "desc_formation = desc_formation.apply(lambda line: gensim.utils.simple_preprocess(line))\n",
    "obj_formation = obj_formation.apply(lambda line: gensim.utils.simple_preprocess(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrinting processed descriptions (as tokens) to a file\n",
    "\n",
    "begin = time.time()\n",
    "desc_formation.to_csv('./data/20200408_Processed_Data/20200410_Processed_Descriptions_Tokens_NewLO.csv',sep=';')\n",
    "end = time.time()\n",
    "\n",
    "print(\"Done in {} seconds\".format(round(end-begin,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrinting processed objectives (still as a text) to a file\n",
    "\n",
    "begin = time.time()\n",
    "obj_formation.to_csv('./data/20200408_Processed_Data/20200410_Processed_Objectives_Tokens_NewLO.csv',sep=';')\n",
    "end = time.time()\n",
    "\n",
    "print(\"Done in {} seconds\".format(round(end-begin,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<hr>\n",
    "\n",
    "Author [Guillaume Lefebvre](https://www.linkedin.com/in/guillaume-lefebvre-22117610b/) - For more information, contact us at contact@inokufu.com - Copyright &copy; 2020 [Inokufu](http://www.inokufu.com)\n",
    "\n",
    "<a href=\"http://www.inokufu.com\"><img src = \"http://www.inokufu.com/wp-content/uploads/elementor/thumbs/logo_inokufu_vector_full-black-om2hmu9ob1jytetxemkj1ij8g7tt3hzrtssivh2fl2.png\" width = 400> </a>\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
