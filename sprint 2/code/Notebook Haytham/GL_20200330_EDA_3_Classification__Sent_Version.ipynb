{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<a href=\"http://www.inokufu.com\"><img src = \"http://www.inokufu.com/wp-content/uploads/elementor/thumbs/logo_inokufu_vector_full-black-om2hmu9ob1jytetxemkj1ij8g7tt3hzrtssivh2fl2.png\" width = 400> </a>\n",
    "\n",
    "\n",
    "<h1 align=center><font size = 5>Exploratory Data Analysis : Titre</font></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this notebook, we conduct an Exploratory Data Analysis (EDA). The idea is to better understand which kind of classifier would be the greatest one with the data we processed in part 1, and the model we created in part 2. \n",
    "\n",
    "Our EDA approach follows the **Data Science Methodology CRISP-DM**. For more info about this approach, check this [Wikipedia page](https://en.wikipedia.org/wiki/Cross-industry_standard_process_for_data_mining)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "<div class=\"alert alert-block alert-info\" style=\"margin-top: 20px\">\n",
    "\n",
    "<font size = 3>\n",
    "\n",
    "1. <a href=\"#item1\">Data Collection</a>\n",
    "\n",
    "2. <a href=\"#item2\">Creation of models</a>\n",
    "\n",
    "3. <a href=\"#item3\">Conclusion</a>    \n",
    "\n",
    "</font>\n",
    "</div>\n",
    "<a id='the_destination'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "np.set_printoptions(threshold=10000,suppress=True) \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.colors as colors\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import matplotlib.image as img\n",
    "from matplotlib import rcParams\n",
    "\n",
    "import json\n",
    "import unicodedata\n",
    "\n",
    "import seaborn as sns\n",
    "from cycler import cycler\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import spacy\n",
    "import nltk\n",
    "import os\n",
    "import string\n",
    "import numpy as np\n",
    "import copy\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import re\n",
    "import math\n",
    "import gensim\n",
    "import time \n",
    "import multiprocess\n",
    "import multiprocessing\n",
    "\n",
    "import fr_core_news_sm\n",
    "\n",
    "from spacy_langdetect import LanguageDetector\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, recall_score, precision_score, roc_auc_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfTransformer\n",
    "from sklearn.feature_extraction import stop_words\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, BaggingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import cross_val_score,cross_val_predict, KFold\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "\n",
    "from textblob import Blobber\n",
    "from textblob_fr import PatternTagger, PatternAnalyzer\n",
    "\n",
    "print('Libraries imported.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Import <a id='item1'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './data/20200330_Processed_Data/'\n",
    "file_name = '20200330_Processed_Udemy_Json'\n",
    "file_extension = '.csv'\n",
    "\n",
    "df = pd.read_csv(path + file_name + file_extension, sep=';')\n",
    "df = df.drop('Unnamed: 0', axis=1)\n",
    "df_json = df.reset_index(drop=True)\n",
    "df_json.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting data into \n",
    "\n",
    "corpus = df_json['description']\n",
    "title = df_json['title']\n",
    "Y = df_json['rating_01'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = corpus.apply(lambda line: gensim.utils.simple_preprocess(str(line)))\n",
    "title = title.apply(lambda line: gensim.utils.simple_preprocess(str(line)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Models Import <a id='item2'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the 4 models we trained in the part 2 : Word2Vec\n",
    "\n",
    "model_100_5 = gensim.models.Word2Vec.load(\"./models/20200330_UdemyDesc_Desc_Obj_100_5000_Train25_Iter10.model\")\n",
    "model_100_10 = gensim.models.Word2Vec.load(\"./models/20200330_UdemyDesc_Desc_Obj_100_10000_Train25_Iter10.model\")\n",
    "model_300_5 = gensim.models.Word2Vec.load(\"./models/20200330_UdemyDesc_Desc_Obj_300_5000_Train25_Iter10.model\")\n",
    "model_300_10 = gensim.models.Word2Vec.load(\"./models/20200330_UdemyDesc_Desc_Obj_300_10000_Train25_Iter10.model\")\n",
    "\n",
    "model_to_send = gensim.models.Word2Vec.load(\"./models/Word2Vec_model_to_send.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Transformation <a id='item3'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting word2vec vector for each sentences : AVERAGE word Embedding\n",
    "# The variable type represents the type of the embedding : 'average' or 'sum'\n",
    "\n",
    "def word2vec_Embedding(reviews_unigram, model_, model_size_, type_):\n",
    "    \n",
    "    # If the type doesn't exist, quit the function \n",
    "    if type_ != 'average':\n",
    "        if type_ != 'sum':\n",
    "            print(\"Wrong type selected\")\n",
    "            return\n",
    "    \n",
    "    dict_word2vec = {}\n",
    "    for index, word_list in enumerate(reviews_unigram):\n",
    "        arr = np.array([0.0 for i in range(0, model_size_)])\n",
    "        for word in word_list:\n",
    "            try:\n",
    "                arr += model_.wv[word]\n",
    "            except KeyError:\n",
    "                continue\n",
    "                \n",
    "        if type_ == 'average':\n",
    "            # Doing the mean of vectors :\n",
    "            if(len(word_list) == 0):\n",
    "                dict_word2vec[index] = arr\n",
    "            else:\n",
    "                dict_word2vec[index] = arr / len(word_list)\n",
    "        elif type_ == 'sum':\n",
    "            # Doing the sum of vectors :\n",
    "            dict_word2vec[index] = arr\n",
    "        \n",
    "    df_word2vec = pd.DataFrame(dict_word2vec).T\n",
    "    return df_word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting each sentence to a vector using Word2Vec and word embedding : Average and Sum\n",
    "\n",
    "# Trying on this model : \n",
    "model = model_300_5\n",
    "model_size = 300\n",
    "\n",
    "df_word2vec = word2vec_Embedding(corpus, model, model_size, 'average');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_word2vec.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Classification <a id='item4'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instanciation des classifieurs\n",
    "clfs = {\n",
    "    'RF': RandomForestClassifier(n_estimators = 500, random_state = 0),\n",
    "    #'ADA': AdaBoostClassifier(n_estimators=50),\n",
    "    #'BAG': BaggingClassifier(n_estimators=50),\n",
    "    #'KNN': KNeighborsClassifier(n_neighbors=5),\n",
    "    #'NB': GaussianNB(),\n",
    "    #'MLP': MLPClassifier(hidden_layer_sizes=(20, 10), alpha=0.001, max_iter=200),\n",
    "    #'CART': DecisionTreeClassifier(criterion='gini'),\n",
    "    #'ID3': DecisionTreeClassifier(criterion='entropy'),\n",
    "    #'ST': DecisionTreeClassifier(max_depth=1)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_classifiers (X,Y,clfs):\n",
    "    \n",
    "    kf = KFold(n_splits=10, shuffle=True, random_state=0)\n",
    "    \n",
    "    for clf_name in clfs:\n",
    "        clf = clfs[clf_name]\n",
    "        \n",
    "        begin = time.time()\n",
    "        cv_acc = cross_val_score(clf, X, Y, cv=kf)\n",
    "        end = time.time()\n",
    "        print(\"Accuracy for {0} is: {1:.3f} +/- {2:.3f} (in {3:.2f} seconds)\".format(clf_name, np.mean(cv_acc), np.std(cv_acc), end-begin))\n",
    "        \n",
    "        cv_precision = cross_val_score(clf, X, Y, cv=kf, scoring='precision') \n",
    "        print(\"Precision for {0} is: {1:.3f} +/- {2:.3f}\".format(clf_name, np.mean(cv_precision), np.std(cv_precision)))\n",
    "        \n",
    "        cv_recall = cross_val_score(clf, X, Y, cv=kf, scoring='recall')\n",
    "        print(\"Recall for {0} is: {1:.3f} +/- {2:.3f}\".format(clf_name, np.mean(cv_recall), np.std(cv_recall)))\n",
    "        \n",
    "        cv_auc = cross_val_score(clf, X, Y, cv=kf, scoring='roc_auc') \n",
    "        print(\"AUC for {0} is: {1:.3f} +/- {2:.3f}\".format(clf_name, np.mean(cv_auc), np.std(cv_auc)))\n",
    "        \n",
    "        Y_pred = cross_val_predict(clf, X, Y, cv=kf)\n",
    "        \n",
    "        cohen_kappa = cohen_kappa_score(Y,Y_pred)\n",
    "        print(\"Cohen-Kappa for {0} is: {1:.3f}\".format(clf_name, cohen_kappa))\n",
    "        \n",
    "        conf_mat = confusion_matrix(Y, Y_pred)\n",
    "        print(conf_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_classifiers(df_word2vec, Y, clfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add variables : price_detail and num_sub "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concat df_word2vec avec les nouvelles variables \n",
    "\n",
    "df_final = pd.concat([df_word2vec,df_json['price_detail'],df_json['num_sub']],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_final.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "run_classifiers(df_final, Y, clfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Normalisation + ACP <a id='item5'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "sc = StandardScaler()\n",
    "sc.fit(X)\n",
    "X_norm = sc.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ajout d'une ACP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Test : le but est de savoir la meilleure quantité de colonnes à conserver pour que l'ACP soit efficace\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA()\n",
    "pca.fit(X_norm)\n",
    "\n",
    "n = len(X_norm)\n",
    "p = len(df_final.columns)\n",
    "\n",
    "eigval = (n-1)/n * pca.explained_variance_\n",
    "\n",
    "plt.plot(np.arange(1,p+1),eigval) \n",
    "plt.title(\"Scree plot\") \n",
    "plt.ylabel(\"Eigen values\") \n",
    "plt.xlabel(\"Factor number\") \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA()\n",
    "pca.fit(X_norm)\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.xlabel('number of components')\n",
    "plt.ylabel('cumulative explained variance');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components = 6)\n",
    "pca.fit(X_norm)\n",
    "\n",
    "X_pca = np.concatenate((X_norm, pca.transform(X_norm)), axis=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "run_classifiers(X_pca, Y, clfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Next Steps <a id='item6'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we have done so far is quite good : we actually have a model with 67% accuracy and 69% precision, which is not that bad compared to the naive classifier (which has 50% accuracy).   \n",
    "\n",
    "For next steps, we could :\n",
    "- try other word2vec models with different parameters\n",
    "- try to change the number of columns we keep from PCA (actually 17)\n",
    "- try other classifiers with other params : we tried many classifiers, but with no optimisation on param's choices\n",
    "- try Bert, LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<hr>\n",
    "\n",
    "Author [Guillaume Lefebvre](https://www.linkedin.com/in/guillaume-lefebvre-22117610b/) - For more information, contact us at contact@inokufu.com - Copyright &copy; 2020 [Inokufu](http://www.inokufu.com)\n",
    "\n",
    "<a href=\"http://www.inokufu.com\"><img src = \"http://www.inokufu.com/wp-content/uploads/elementor/thumbs/logo_inokufu_vector_full-black-om2hmu9ob1jytetxemkj1ij8g7tt3hzrtssivh2fl2.png\" width = 400> </a>\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
