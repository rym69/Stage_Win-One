{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<a href=\"http://www.inokufu.com\"><img src = \"http://www.inokufu.com/wp-content/uploads/elementor/thumbs/logo_inokufu_vector_full-black-om2hmu9ob1jytetxemkj1ij8g7tt3hzrtssivh2fl2.png\" width = 400> </a>\n",
    "\n",
    "\n",
    "<h1 align=center><font size = 5>Exploratory Data Analysis : Titre</font></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this notebook, we conduct an Exploratory Data Analysis (EDA) on data visualization. The idea is to better understand how the differents models worked on words : are the models good representations of the real life ? Are the models correctly identifying bloom verbs ? \n",
    "\n",
    "Our EDA approach follows the **Data Science Methodology CRISP-DM**. For more info about this approach, check this [Wikipedia page](https://en.wikipedia.org/wiki/Cross-industry_standard_process_for_data_mining)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "<div class=\"alert alert-block alert-info\" style=\"margin-top: 20px\">\n",
    "\n",
    "<font size = 3>\n",
    "\n",
    "1. <a href=\"#item1\">Data Import</a>\n",
    "\n",
    "2. <a href=\"#item2\">Functions Declarations</a>\n",
    "\n",
    "3. <a href=\"#item3\">Data Visualization</a>    \n",
    "    \n",
    "4. <a href=\"#item4\">Next Steps</a>    \n",
    "\n",
    "</font>\n",
    "</div>\n",
    "<a id='the_destination'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "np.set_printoptions(threshold=10000,suppress=True) \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.colors as colors\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import matplotlib.image as img\n",
    "from matplotlib import rcParams\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "import gensim\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    " \n",
    "import seaborn as sns\n",
    "sns.set_style(\"darkgrid\")\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "import import_ipynb\n",
    "from GL_20200327_Fonctions_Preprocessing import preprocess \n",
    "\n",
    "\n",
    "\n",
    "print('Libraries imported.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Import <a id='item1'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the 4 models we trained in the part 2 : Word2Vec\n",
    "\n",
    "model_100_5 = {\n",
    "    'model' : gensim.models.Word2Vec.load(\"./models/20200330_UdemyDesc_Desc_Obj_100_5000_Train25_Iter10.model\"),\n",
    "    'model_size' : 100, 'vocab_size' : 5000, 'train' : 25, 'iter' : 10\n",
    "}\n",
    "\n",
    "model_100_10 = {\n",
    "    'model' : gensim.models.Word2Vec.load(\"./models/20200330_UdemyDesc_Desc_Obj_100_10000_Train25_Iter10.model\"),\n",
    "    'model_size' : 100, 'vocab_size' : 10000, 'train' : 25, 'iter' : 10\n",
    "}\n",
    "\n",
    "model_300_5 = {\n",
    "    'model' : gensim.models.Word2Vec.load(\"./models/20200330_UdemyDesc_Desc_Obj_300_5000_Train25_Iter10.model\"),\n",
    "    'model_size' : 300, 'vocab_size' : 5000, 'train' : 25, 'iter' : 10\n",
    "}\n",
    "\n",
    "model_300_10 = {\n",
    "    'model' : gensim.models.Word2Vec.load(\"./models/20200330_UdemyDesc_Desc_Obj_300_10000_Train25_Iter10.model\"),\n",
    "    'model_size' : 300, 'vocab_size' : 10000, 'train' : 25, 'iter' : 10\n",
    "}\n",
    "\n",
    "model_to_send = {\n",
    "    'model' : gensim.models.Word2Vec.load(\"./models/Word2Vec_model_to_send.model\"),\n",
    "    'model_size' : 300, 'vocab_size' : 5000, 'train' : 100, 'iter' : 0\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Functions declarations <a id='item2'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prints closest and most far words\n",
    "\n",
    "def tsnescatterplot(model, word, list_names, model_size):\n",
    "\n",
    "    arrays = np.empty((0, model_size), dtype='f')\n",
    "    word_labels = [word]\n",
    "    color_list  = ['red']\n",
    "\n",
    "    # adds the vector of the query word\n",
    "    arrays = np.append(arrays, model.wv.__getitem__([word]), axis=0)\n",
    "    \n",
    "    # gets list of most similar words\n",
    "    close_words = model.wv.most_similar([word])\n",
    "    \n",
    "    # adds the vector for each of the closest words to the array\n",
    "    for wrd_score in close_words:\n",
    "        wrd_vector = model.wv.__getitem__([wrd_score[0]])\n",
    "        word_labels.append(wrd_score[0])\n",
    "        color_list.append('blue')\n",
    "        arrays = np.append(arrays, wrd_vector, axis=0)\n",
    "    \n",
    "    # adds the vector for each of the words from list_names to the array\n",
    "    for wrd in list_names:\n",
    "        wrd_vector = model.wv.__getitem__([wrd])\n",
    "        word_labels.append(wrd)\n",
    "        color_list.append('green')\n",
    "        arrays = np.append(arrays, wrd_vector, axis=0)\n",
    "        \n",
    "    # Reduces the dimensionality from model_size to 50 dimensions with PCA\n",
    "    reduc = PCA(n_components=19).fit_transform(arrays)\n",
    "    \n",
    "    # Finds t-SNE coordinates for 2 dimensions\n",
    "    np.set_printoptions(suppress=True)\n",
    "    \n",
    "    Y = TSNE(n_components=2, random_state=0, perplexity=15).fit_transform(reduc)\n",
    "    \n",
    "    # Sets everything up to plot\n",
    "    df = pd.DataFrame({'x': [x for x in Y[:, 0]], 'y': [y for y in Y[:, 1]], \n",
    "                       'words': word_labels, 'color': color_list})\n",
    "    \n",
    "    fig, _ = plt.subplots()\n",
    "    fig.set_size_inches(9, 9)\n",
    "    \n",
    "    # Basic plot\n",
    "    p1 = sns.regplot(data=df,x=\"x\",y=\"y\",fit_reg=False,marker=\"o\",scatter_kws={'s': 40,'facecolors': df['color']})\n",
    "    \n",
    "    # Adds annotations one by one with a loop\n",
    "    for line in range(0, df.shape[0]):\n",
    "         p1.text(df[\"x\"][line], df['y'][line], '  ' + df[\"words\"][line].title(), horizontalalignment='left',\n",
    "                 verticalalignment='bottom', size='medium',color=df['color'][line],weight='normal').set_size(15)\n",
    "\n",
    "    plt.xlim(Y[:, 0].min()-50, Y[:, 0].max()+50)\n",
    "    plt.ylim(Y[:, 1].min()-50, Y[:, 1].max()+50)\n",
    "            \n",
    "    plt.title('t-SNE visualization for {}'.format(word.title()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prints nearest words only \n",
    "\n",
    "def tsnescatterplot_near_only(model, word, model_size):\n",
    "    \n",
    "    arrays = np.empty((0, model_size), dtype='f')\n",
    "    word_labels = [word]\n",
    "    color_list  = ['red']\n",
    "\n",
    "    # adds the vector of the query word\n",
    "    arrays = np.append(arrays, model.wv.__getitem__([word]), axis=0)\n",
    "    \n",
    "    # gets list of most similar words\n",
    "    close_words = model.wv.most_similar([word])\n",
    "    \n",
    "    # adds the vector for each of the closest words to the array\n",
    "    for wrd_score in close_words:\n",
    "        wrd_vector = model.wv.__getitem__([wrd_score[0]])\n",
    "        word_labels.append(wrd_score[0])\n",
    "        color_list.append('blue')\n",
    "        arrays = np.append(arrays, wrd_vector, axis=0)\n",
    "        \n",
    "    reduc = PCA(n_components=9).fit_transform(arrays)\n",
    "    \n",
    "    # Finds t-SNE coordinates for 2 dimensions\n",
    "    np.set_printoptions(suppress=True)\n",
    "    \n",
    "    Y = TSNE(n_components=2, random_state=0, perplexity=15).fit_transform(reduc)\n",
    "    \n",
    "    # Sets everything up to plot\n",
    "    df = pd.DataFrame({'x': [x for x in Y[:, 0]], 'y': [y for y in Y[:, 1]],\n",
    "                       'words': word_labels, 'color': color_list})\n",
    "    \n",
    "    fig, _ = plt.subplots()\n",
    "    fig.set_size_inches(6, 6)\n",
    "    \n",
    "    # Basic plot\n",
    "    p1 = sns.regplot(data=df,x=\"x\",y=\"y\",fit_reg=False,marker=\"o\",scatter_kws={'s': 40,'facecolors': df['color']})\n",
    "    \n",
    "    # Adds annotations one by one with a loop\n",
    "    for line in range(0, df.shape[0]):\n",
    "         p1.text(df[\"x\"][line], df['y'][line], '  ' + df[\"words\"][line].title(), horizontalalignment='left',\n",
    "                 verticalalignment='bottom', size='medium', color=df['color'][line], weight='normal').set_size(15)\n",
    "    \n",
    "    plt.xlim(Y[:, 0].min()-50, Y[:, 0].max()+50)\n",
    "    plt.ylim(Y[:, 1].min()-50, Y[:, 1].max()+50)\n",
    "            \n",
    "    plt.title('t-SNE visualization for {}'.format(word.title()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    " \n",
    "import seaborn as sns\n",
    "sns.set_style(\"darkgrid\")\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "def tsnescatterplot_vocab(model, model_size, top):\n",
    "    \n",
    "    word_labels = []\n",
    "    color_list  = []\n",
    "    \n",
    "    vocab = model.wv.vocab.items()\n",
    "    array = []\n",
    "    arrays = np.empty((0, model_size), dtype='f')\n",
    "    words = np.empty((0, model_size), dtype='f')\n",
    "    \n",
    "    for wrd_score in vocab:\n",
    "        word = wrd_score[0]\n",
    "        wrd_count = model.wv.vocab[word].count\n",
    "        wrd_vector = model.wv.__getitem__([word])\n",
    "        \n",
    "        word_labels.append(wrd_score[0])\n",
    "        color_list.append('green')\n",
    "        array.append([word,wrd_count,wrd_vector])\n",
    "        \n",
    "    df_test_array = pd.DataFrame(array)\n",
    "    df_test_array.columns = ['word','count','vector']\n",
    "    df_test_array.sort_values(by=['count'], inplace=True, ascending=False)\n",
    "    \n",
    "    for index,row in df_test_array.iterrows():\n",
    "        arrays = np.append(arrays, row['vector'], axis=0)\n",
    "        words = np.append(words, row['word'])\n",
    "    \n",
    "    reduc = PCA(n_components=19).fit_transform(arrays[:top])\n",
    "    np.set_printoptions(suppress=True)\n",
    "    Y = TSNE(n_components=2, random_state=0, perplexity=15).fit_transform(reduc)\n",
    "    df = pd.DataFrame({'x': [x for x in Y[:, 0]], 'y': [y for y in Y[:, 1]], 'words': words[:top]})\n",
    "    \n",
    "    fig, _ = plt.subplots()\n",
    "    fig.set_size_inches(9, 9)\n",
    "    \n",
    "    # Basic plot\n",
    "    p1 = sns.regplot(data=df, x=\"x\", y=\"y\", fit_reg=False, marker=\"o\", scatter_kws={'s': 40,'facecolors': 'green'})\n",
    "    \n",
    "    # Adds annotations one by one with a loop\n",
    "    for line in range(0, df.shape[0]):\n",
    "         p1.text(df[\"x\"][line],df['y'][line],'  ' + df[\"words\"][line].title(),horizontalalignment='left',\n",
    "                 verticalalignment='bottom', size='medium',color='green',weight='normal').set_size(15)\n",
    "    \n",
    "    plt.xlim(Y[:, 0].min()-50, Y[:, 0].max()+50)\n",
    "    plt.ylim(Y[:, 1].min()-50, Y[:, 1].max()+50)\n",
    "            \n",
    "    plt.title('t-SNE visualization')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data visualization <a id='item3'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model_300_5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prints the 10 closest words, and the 10 most far words, to the word we test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_tested = 'decouvr'\n",
    "\n",
    "tsnescatterplot(model['model'], \n",
    "                word_tested, \n",
    "                [i[0] for i in model['model'].wv.most_similar(negative=[word_tested])], \n",
    "                model['model_size'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prints top N most used words of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 25\n",
    "tsnescatterplot_vocab(model['model'],model['model_size'],N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for the nearest words to bloom verbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "words = 'decouvrir expliquer construire analyser synthetiser evaluer'\n",
    "words = preprocess(words)\n",
    "words = words.split(\" \")\n",
    "\n",
    "for word_tested in words:\n",
    "    tsnescatterplot_near_only(model['model'], word_tested, model['model_size'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bloom verbs analysis only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Original bloom list : \n",
    "\n",
    "[['Connaitre','Définir','Citer','Nommer','Décrire','Dire','Relater','Réciter','Désigner','Montrer','Énoncer','Énumérer','Identifier','Lister','Insérer','Ordonner','Arranger','Localiser','Placer','Situer','Souligner','Dupliquer','Rappeler','Reconnaitre'],\n",
    "    ['Comprendre','Reformuler','Paraphraser','Démontrer','Différencier','Discriminer','Expliquer','Retracer','Formuler','Intégrer','Interpréter','Résumer','Rapporter','Associer','Relier','Regrouper','Classifier','Estimer','Réviser','Traduire','Discuter'],\n",
    "    ['Appliquer','Faire','Utiliser','Exercer','Employer','Administrer','Adapter','Planifier','Opérer','Calculer','Experimenter','Simuler','Préparer','Pratiquer','Produire','Construire','Interviewer'],\n",
    "    ['Analyser','Etudier','Examiner','Décomposer','Disséquer','Extraire','Rechercher','Comparer','Critiquer','Catégoriser','Contraster','Corréler','Subdiviser','Prioriser','Schématiser','Concentrer','Signaler','Regrouper'],\n",
    "    ['Synthétiser','Construire','Modifier','Assembler','Compiler','Composer','Edifier','Façonner','Intégrer','Recombiner','Réorganiser','Réordonner','Reconstruire','Structurer','Systématiser','Créer','Concevoir','Développer','Adapter','Généraliser','Imaginer','Inventer'],\n",
    "    ['Evaluer','Prédire','Estimer','Expertiser','Juger','Sélectionner','Choisir','Prédire','Justifier','Recommander','Noter','Décider','Défendre','Persuader']\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition of the bloom verbs list without duplicated verbs\n",
    "\n",
    "bloom_array = [\n",
    "    ['Connaitre','Définir','Citer','Nommer','Décrire','Dire','Relater','Réciter','Désigner','Montrer','Énoncer','Énumérer','Identifier','Lister','Insérer','Ordonner','Arranger','Localiser','Placer','Situer','Souligner','Dupliquer','Rappeler','Reconnaitre'],\n",
    "    ['Comprendre','Reformuler','Paraphraser','Démontrer','Différencier','Discriminer','Expliquer','Retracer','Formuler','Interpréter','Résumer','Rapporter','Associer','Relier','Classifier','Réviser','Traduire','Discuter'],\n",
    "    ['Appliquer','Faire','Utiliser','Exercer','Employer','Administrer','Planifier','Opérer','Calculer','Experimenter','Simuler','Préparer','Pratiquer','Produire','Interviewer'],\n",
    "    ['Analyser','Etudier','Examiner','Décomposer','Disséquer','Extraire','Rechercher','Comparer','Critiquer','Catégoriser','Contraster','Corréler','Subdiviser','Prioriser','Schématiser','Concentrer','Signaler'],\n",
    "    ['Synthétiser','Modifier','Assembler','Compiler','Composer','Edifier','Façonner','Recombiner','Réorganiser','Réordonner','Reconstruire','Structurer','Systématiser','Créer','Concevoir','Développer','Généraliser','Imaginer','Inventer'],\n",
    "    ['Evaluer','Prédire','Expertiser','Juger','Sélectionner','Choisir','Prédire','Justifier','Recommander','Noter','Décider','Défendre','Persuader']\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concat verbs by category\n",
    "\n",
    "bloom_list = []\n",
    "\n",
    "for i in range(len(bloom_array)):\n",
    "    bloom_sentence = ' '.join(bloom_array[i])\n",
    "    bloom_list.append(bloom_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print verbs of the first categ\n",
    "bloom_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process verbs by category : we keep the processed verbs by string of many verbs, and by array of verbs by categ\n",
    "\n",
    "bloom_list_processed = []\n",
    "bloom_array_processed = []\n",
    "\n",
    "for i in range(len(bloom_list)):\n",
    "    processed_bloom = preprocess(bloom_list[i])\n",
    "    bloom_list_processed.append(processed_bloom)\n",
    "    bloom_array_processed.append(processed_bloom.split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print processed sentences composed of verbs from the first categ\n",
    "bloom_list_processed[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print processed words from the first category\n",
    "print(bloom_array_processed[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate distances between verbs of the same category \n",
    "\n",
    "bloom_categ = []\n",
    "bloom_verb1 = []\n",
    "bloom_verb2 = []\n",
    "bloom_distance = []\n",
    "bloom_not_vocab = []\n",
    "\n",
    "# For each category \n",
    "for i in range(len(bloom_array_processed)):\n",
    "    \n",
    "    # For each verb of each category\n",
    "    for j in range(len(bloom_array_processed[i])):\n",
    "        \n",
    "        # For each verb after the j verb\n",
    "        for k in range(j+1,len(bloom_array_processed[i])):\n",
    "            \n",
    "            # Get verbs we have to calculate the distance between\n",
    "            verb1 = bloom_array_processed[i][j]\n",
    "            verb2 = bloom_array_processed[i][k]\n",
    "            \n",
    "            # Check if verbs are in the word2vec model\n",
    "            if verb1 in model['model'].wv.vocab and verb2 in model['model'].wv.vocab:\n",
    "                \n",
    "                # If yes, calculate the distance between them\n",
    "                distance = model['model'].similarity(verb1,verb2)\n",
    "            else :\n",
    "                #If no:\n",
    "                \n",
    "                # Distance is -1\n",
    "                distance = -1\n",
    "                \n",
    "                # We add the verb that is not in vocab into an array\n",
    "                if verb1 in model['model'].wv.vocab:\n",
    "                    bloom_not_vocab.append(verb2)\n",
    "                else:\n",
    "                    bloom_not_vocab.append(verb1)\n",
    "            \n",
    "            # We add categories, verbs and distances into arrays\n",
    "            bloom_categ.append(i)\n",
    "            bloom_verb1.append(verb1)\n",
    "            bloom_verb2.append(verb2)\n",
    "            bloom_distance.append(distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concat of all the arrays we just filled into a Pandas DataFrame\n",
    "df = pd.DataFrame([bloom_categ, bloom_verb1, bloom_verb2, bloom_distance]).transpose()\n",
    "\n",
    "# Add the columns of the dataframe \n",
    "df.columns = [\"categ\",\"verb_1\",\"verb_2\",\"distance\"]\n",
    "\n",
    "# Sort the dataframe by distance descending, and print the 10 first rows (highest distances)\n",
    "df.sort_values('distance',ascending = False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verbs that are not in the model vocabulary\n",
    "\n",
    "df_not_vocab = pd.DataFrame([bloom_not_vocab]).transpose()\n",
    "list_verb_not_vocab = df_not_vocab[0].unique()\n",
    "list_verb_not_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check which categories are badly represented\n",
    "df_negativ = df.loc[df['distance'] < 0]\n",
    "df_positiv = df.loc[df['distance'] >= 0]\n",
    "\n",
    "list_positiv_by_categ = df_positiv['categ'].value_counts().sort_index()\n",
    "list_negativ_by_categ = df_negativ['categ'].value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Positiv\",\"\\t\",\"Negativ\",\"\\t\",\"Rate\")\n",
    "for i in range(6):\n",
    "    rate = round(list_positiv_by_categ[i]/(list_positiv_by_categ[i]+list_negativ_by_categ[i])*100,2)\n",
    "    print(list_positiv_by_categ[i],\"\\t\\t\",list_negativ_by_categ[i],\"\\t\\t\",rate,\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This small analysis seems to show that : \n",
    "- categories 1,2 and 3 are well represented by the model : More than half of them are detected as close\n",
    "- categories 4,5 and 6 are badly represented by the model : Less than half of them are detected as close\n",
    "\n",
    "This could be explained by the fact that : \n",
    "- categories don't have the same number of bloom verbs \n",
    "- some verbs can be used for other reasons than education  \n",
    "\n",
    "--> Example : \"Ca va vous faire du bien de voir cette compétence\" : here, \"Faire\" is the only bloom verb in this sentence, but he is not saying anything about learning/teaching. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build an array with all verbs and their categories\n",
    "\n",
    "bloom_array_processed_all = []\n",
    "\n",
    "for i in range(len(bloom_array_processed)):\n",
    "    for j in range(len(bloom_array_processed[i])):\n",
    "        bloom_array_processed_all.append([i,bloom_array_processed[i][j]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(bloom_array_processed_all[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distances between verbs from all categories \n",
    "# --> Goal : see if there are verbs from different categories that are close in distance\n",
    "\n",
    "bloom_categ1 = []\n",
    "bloom_verb1 = []\n",
    "bloom_categ2 = []\n",
    "bloom_verb2 = []\n",
    "bloom_distance = []\n",
    "\n",
    "for i in range(len(bloom_array_processed_all)):\n",
    "    for j in range(i+1,len(bloom_array_processed_all)):\n",
    "        verb1 = bloom_array_processed_all[i][1]\n",
    "        verb2 = bloom_array_processed_all[j][1]\n",
    "        \n",
    "        if verb1 in model['model'].wv.vocab and verb2 in model['model'].wv.vocab:\n",
    "            distance = model['model'].similarity(verb1,verb2)\n",
    "        else :\n",
    "            distance = -1\n",
    "            \n",
    "        bloom_categ1.append(bloom_array_processed_all[i][0])\n",
    "        bloom_verb1.append(verb1)\n",
    "        bloom_categ2.append(bloom_array_processed_all[j][0])\n",
    "        bloom_verb2.append(verb2)\n",
    "        bloom_distance.append(distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concat of all the arrays we just filled into a Pandas DataFrame\n",
    "df_all = pd.DataFrame([bloom_categ1, bloom_verb1, bloom_categ2, bloom_verb2, bloom_distance]).transpose()\n",
    "\n",
    "# Add the columns of the dataframe \n",
    "df_all.columns = [\"categ_1\", \"verb_1\", \"categ_2\", \"verb_2\", \"distance\"]\n",
    "\n",
    "# Sort the dataframe by distance descending, and print the 10 first rows (highest distances)\n",
    "df_all.sort_values('distance',ascending = False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicated verbs : distance between same words = 1\n",
    "df_all = df_all.loc[df_all['verb_1'] != df_all['verb_2']]\n",
    "\n",
    "# Remove not in vocab words\n",
    "df_all = df_all.loc[df_all['distance'] != -1]\n",
    "\n",
    "print(len(df_all))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Close Verbs Quick Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only the close words\n",
    "df_positiv = df_all[df_all['distance'] >= 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_positiv.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Number of verbs that are close and in the same category \n",
    "well = len(df_positiv[df_positiv['categ_1'] == df_positiv['categ_2']])\n",
    "\n",
    "# Number of verbs that are close and not in the same category\n",
    "bad = len(df_positiv[df_positiv['categ_1'] != df_positiv['categ_2']])\n",
    "\n",
    "# Rate of close words of the same category on close words\n",
    "print(\"Rate of verbs well interpreted as close :\")\n",
    "print(round(well/len(df_positiv),4)*100,\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Far Verbs Quick Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only the far words \n",
    "df_negativ = df_all.loc[df_all['distance'] < 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_negativ.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of verbs that are far and in the same category \n",
    "bad = len(df_negativ[df_negativ['categ_1'] == df_negativ['categ_2']])\n",
    "\n",
    "# Number of verbs that are far and not in the same category\n",
    "well = len(df_negativ[df_negativ['categ_1'] != df_negativ['categ_2']])\n",
    "\n",
    "# Rate of far words of the same category on far words\n",
    "print(\"Rate of verbs well interpreted as far :\")\n",
    "print(round(well/len(df_negativ),4)*100,\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_positiv[df_positiv['categ_1'] != df_positiv['categ_2']].sort_values('distance',ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Next Steps <a id='item4'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This analysis on bloom verbs shows that verbs from the same category are not always said as close.\n",
    "On the opposite, this analysis shows that verbs from different categories are most of the time detected as \"far\". \n",
    "\n",
    "This could be explained by the fact that : \n",
    "- categories have a lot of verbs,\n",
    "- some of the verbs might not be used enough to teach the model,\n",
    "- some verbs can be used for other reasons than education, which might misteach to the model\n",
    "\n",
    "After this analysis, we could think that, the model is not learning enough from the descriptions and objectives.\n",
    "This might be a new thing to think about : would a classifier be more accurate if we added the TF-IDF variables on bloom verbs ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<hr>\n",
    "\n",
    "Author [Guillaume Lefebvre](https://www.linkedin.com/in/guillaume-lefebvre-22117610b/) - For more information, contact us at contact@inokufu.com - Copyright &copy; 2020 [Inokufu](http://www.inokufu.com)\n",
    "\n",
    "<a href=\"http://www.inokufu.com\"><img src = \"http://www.inokufu.com/wp-content/uploads/elementor/thumbs/logo_inokufu_vector_full-black-om2hmu9ob1jytetxemkj1ij8g7tt3hzrtssivh2fl2.png\" width = 400> </a>\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
